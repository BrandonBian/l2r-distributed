apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: walker-dupdate-workers
  labels:
    tier: walker-dupdate-workers
spec:
  # 2 to start, then for phortx 30-40 is the upper bound probably
  replicas: 3
  selector:
    matchLabels:
      tier: walker-dupdate-workers
  template:
    metadata:
      labels:
        tier: walker-dupdate-workers
    spec:
      nodeSelector:
        # we don't specify the cluster, maybe it's ok to just delete the phortx stuff?
        # 1-3
        nodetype: phortx
      
      containers:
      - name: walker-dupdate-workers
        tty: true
        stdin: true
        resources: # Maybe a good idea to have the learner on its own gpu, specify like below
            limits:
                nvidia.com/gpu: 1
        env:
          - name: NVIDIA_VISIBLE_DEVICES 
            value: "0" 
          - name: CUDA_VISIBLE_DEVICES
            value: "0" 
        image: docker.pdl.cmu.edu/l2r2022:latest
        command: # remember to configure 'AGENT_NAME' and 'TRAINING_PARADIGM'
          - /bin/bash
          - -c
          - export AGENT_NAME=walker &&
              export TRAINING_PARADIGM=distribUpdate &&
              apt-get update &&
              apt-get install xvfb swig -y &&
              pip3 install tianshou gym strictyaml line_profiler &&
              git clone https://github.com/BrandonBian/l2r-distributed.git &&
              cd "$(\ls -1dt ./*/ | head -n 1)" &&
              pip3 install -r setup/devtools_reqs.txt &&
              pip3 install wandb tensorboardX jsonpickle &&
              pip3 install gym[box2d] protobuf==3.20.* &&
              python3 worker.py --env walker --paradigm dUpdate

---
apiVersion: v1
kind: Pod
metadata:
  name: walker-dupdate-learner
  labels:
    app.kubernetes.io/name: proxy
spec:
  hostname: learner-1
  nodeSelector:
    nodetype: phortx
  containers:
    - name: walker-dupdate-learner
      tty: true
      stdin: true
      resources: # Maybe a good idea to have the learner on its own gpu, specify like below
        limits:
          nvidia.com/gpu: 1
      image: docker.pdl.cmu.edu/l2r2022:latest # Slightly different image or files or git repo
      command: # remember to configure 'AGENT_NAME' and 'TRAINING_PARADIGM'
        - /bin/bash
        - -c
        - export AGENT_NAME=walker &&
          export TRAINING_PARADIGM=distribUpdate &&
          pip3 install tianshou gym strictyaml wandb line_profiler &&
          git clone https://github.com/BrandonBian/l2r-distributed.git &&
          cd "$(\ls -1dt ./*/ | head -n 1)" &&
          pip3 install gym[box2d] protobuf==3.20.* &&
          python3 server.py --env walker --paradigm dUpdate --wandb_apikey 173e38ab5f2f2d96c260f57c989b4d068b64fb8a --exp_name walker-dUpdate-3Workers

      ports:
      - name: walker-dupdate
        containerPort: 4444
--- 
apiVersion: v1
kind: Service
metadata:
  name: walker-dupdate-learner
spec:
  selector:
    app.kubernetes.io/name: proxy
  ports:
  - name: walker-dupdate
    protocol: TCP
    port: 4444
    targetPort: walker-dupdate
---






#### Or we can run mulitple experiments by specifying node for a diff experiment


# ---
# apiVersion: apps/v1
# kind: ReplicaSet
# metadata:
#   name: worker-pods
#   labels:
#     tier: worker-set
# spec:
#   # 2 to start, then for phortx 30-40 is the upper bound probably
#   replicas: "{{NUM_WORKERS}}" 
#   selector:
#     matchLabels:
#       tier: worker-set
#   template:
#     metadata:
#       labels:
#         tier: worker-set
#     spec:
#       nodeSelector:
#         # we don't specify the cluster, maybe it's ok to just delete the phortx stuff?
#         # 1-3
#         nodetype: 1
#       containers:
#         - name: worker-container
#           tty: true
#           stdin: true
#           env:
#             - name: NVIDIA_VISIBLE_DEVICES 
#               value: "{{GPU_ID}}" 
#             - name: CUDA_VISIBLE_DEVICES
#               value: "{{GPU_ID}}" 
#           image: "{{WORKER_IMAGE}}"
#           command:
#             - "/bin/bash"
#             - "-c"
#             - "{{WORKER_CMD}}"
